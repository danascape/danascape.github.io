[ { "title": "Notes on Custom ROM Bringup", "url": "/posts/Notes-on-Custom-ROM-Bringup/", "categories": "AOSP", "tags": "Android, AOSP, Frameworks, Vendor", "date": "2026-01-17 00:00:00 +0530", "content": "What this is? A practical dump of things I keep re-learning while bringing up AOSP-based fork. Not a tutorial, more like notes / a checklist I wish I had kept while bringing up custom ROMs over the years. Ain’t gonna write “What is a custom ROM?” Picking the right AOSP tag For a custom ROM bring-up, the first thing I look out for is the Android tag / revision I need to pick. Tags can be found here: https://source.android.com/docs/setup/reference/build-numbers AOSP manifest repo is here: https://android.googlesource.com/platform/manifest The manifest branch is linked with the build number. Usually I pick the revision for a specific Android version which has the maximum number of Pixels supported (since that branch tends to get the most attention and patches). Source push has been a bit weird now though, since a lot is going on from Google’s side, if you are here then you know the news :D. This approach was pretty reliable till Android 15. For Android 16, I just picked the latest available revision. Syncing the source After that, the next step is syncing the source and getting one of my devices ready to build with it. In most use cases, the AOSP source is already fine. The work is mostly about adapting and aligning: device tree platform-side trees (HALs, kernel prebuilts) and whatever is needed for the build to compile and boot If the build compiles but doesn’t boot, it’s usually a device/platform side issue. AOSP is built to support a wide range of devices, it’s not meant to perfectly support your device out of the box. I’ll talk about platform/device-specific problems in a separate post, because this post is focused more on custom ROM bring-up. Building and flashing Once the source is synced and my platform trees are in place, the next step is building and flashing the ROM. This can be an otapackage or an updatepackage depending on what you’re targeting. One recommendation I’d personally give: don’t pick a legacy device for your first builds. Things like: pre-BPF devices pre-dynamic partitions devices …might need extra patchsets in the build system just to boot properly. That stuff becomes a topic on its own, and the goal here is to boot pure AOSP first. Back in the day, I used to maintain pure AOSP trees + patchsets to make them boot on devices. But now, with a full-time job, maintaining all that is honestly a pain, so I don’t want to go deep into that here. First boot: “This is AOSP… and it’s empty” Once the OS boots, it’s pure AOSP with nothing extra added at all. And yeah it looks exactly like that. At this point you start realizing how much OEMs actually do on top of AOSP to reach what we see in things like: OneUI ColorOS Pixel builds So from here, the real “custom ROM work” happens. Deciding what UI/UX you want The next part is deciding what kind of UI/UX and feature-set you want in your ROM. For example: QuickSettings customisation Launcher changes Settings changes extra apps SystemUI tweaks and a lot more AOSP is very generic, so to make the ROM usable for daily driving you basically need to decide what ecosystem you want to continue with. Apps: GMS, microG, or your own set For the application part, I usually see these options: Integrate Pixel GMS package (GApps) Use microG Or pick your own set of apps from ROM projects Some good options people commonly borrow from ROM ecosystems: LineageOS apps, like Aperture, Glimpse, Jelly, Gramophone, etc ParanoidAndroid HelluvaOS This usually solves the “base system apps” problem, because these are apps that most people use daily anyway. Customising UI (the fun and painful part) After the basics, I usually focus on UI customisation first. There are many ROMs out there, but I’ll mention the “sane” ones, the ones that actually have original feature-sets or something that makes them unique. Rebranding the ROM A lot of ROM identity comes from rebranding. Examples: LineageOS Paranoid Android LMODroid Rebranding can happen in multiple places: the flashable zip / build branding package names and overlays versioning shown in Settings → About A simple example of how ROM versioning gets shown in Settings: https://github.com/PVOT-OSS/Settings/commit/fa2e594c818447509d7f71d3304083056961b982 Usually this involves defining custom version properties and reading them in Settings to display ROM name/version properly. Some ROMs also handle this differently (like LMODroid), by using a separate Settings extension package and overlays. Rebranding helps users see what OS they’re actually running. Deciding the feature-set Then comes the main part: listing the features you want. There are loads and loads of features that have been built on top of AOSP for more than a decade, since CyanogenMod days. Over time, features have evolved, been rewritten, and carried forward through multiple Android versions. But honestly, what you pick depends on what kind of ROM you want: heavy UX modification privacy-focused build colorful/custom UI builds or a stable minimal OS with clean UX Personally, I like modifications — but not too heavy. The reason is simple: every extra feature you add becomes something you have to carry forward to the next Android upstream, and that eventually becomes painful. So my aim is always a generic minimal OS, with only a limited set of features that are actually used by users. Most features are exposed through Settings, but some ROMs also add a separate settings extension category to control: SystemUI QS status bar tweaks and other components Releasing the ROM (and letting the community break it) After the core ROM feels usable, the next phase is releasing it and scaling it across devices. Device maintainers in the community are usually keen to try building newer Android versions, and they will report ROM-specific issues pretty quickly. Some devices (Samsung, MediaTek) usually need extra patchsets in the build system to become stable. Kernel build: AOSP vs Lineage approach There’s also the kernel build side of things. LineageOS has its own kernel build system that’s widely used across devices, making it easier to build kernel binaries from source. Pure AOSP usually provides prebuilt solutions, and they also have kernel_build with Bazel to build kernel sources. ParanoidAndroid also distributes it but the kernel is built separately, I do not want to delve in it, since it is CLO (pre: CAF) based fork. Long-term maintenance: patches, users, and rebases Once the ROM gets stable, the real maintenance phase starts: fixing bugs answering users handling device-specific reports keeping everything stable across time And then there are security patches. Whenever a new monthly security patch is out: we merge patches by tracking CVEs across repos bump the security string handle changes that come with the monthly patch Some months are simple. Some months come with bigger changes (QPR1 / QPR2 / QPR3), which gets more fun. And then the final boss: a new Android upstream release. That’s when the real race begins: rebasing patches booting quickly fixing breakage before everyone else does and repeating the whole cycle again Commands I use a lot Not listing every command in existence, just the ones I keep using again and again during bring-up / debugging. Repo / source stuff repo init -u https://android.googlesource.com/platform/manifest -b &lt;branch&gt; repo sync -c -j$(nproc) --force-sync --no-tags --no-clone-bundle repo status repo forall -c \"git status -sb\" Build setup source build/envsetup.sh lunch Builds I mostly do m otapackage m updatepackage m -j$(nproc) Flashing adb reboot bootloader fastboot devices fastboot reboot adb reboot recovery adb reboot Logs adb logcat -b all adb logcat -b kernel adb logcat adb shell dmesg adb shell getprop adb shell getprop | grep -i boot adb shell getprop | grep -i init adb shell getprop | grep -i selinux adb shell getevent adb shell getevent -lp adb shell id adb shell ps -A adb shell top adb shell lsof SElinux adb shell service list adb shell dumpsys adb shell dumpsys activity adb shell dumpsys package adb shell cmd package list packages Closing notes This is basically how I approach custom ROM bring-up these days. There’s no fixed recipe honestly every feature surprises you in a different way. But having a mental checklist like this saves a lot of time, especially when you’re coming back to the same mess after weeks/months. If you’re doing custom ROM bring-up too, you probably know the feeling, sometimes the fix is one line, sometimes it’s 3 days of pain :) Bring-up is pain, but it’s also kinda addictive. I’ll keep updating this post whenever I hit something new (or whenever I forget the same thing again). Contact You can hit me an email, if you have questions or anything else at saalim.priv@gmail.com" }, { "title": "GSoC 2025 - Final Journey", "url": "/posts/GSoC-2025-final/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-11-10 00:00:00 +0530", "content": "Summary: The integration of the Robot Operating System (ROS) Framework as an option within Automotive Grade Linux (AGL) through the meta-ros yocto layer is an important step in upgrading vehicle technology using automotive systems. This project is important for several reasons: Bridging the Gap Between Robotics and Automotive Software. Expanding AGL’s Capabilities Advancing Open-Source Development Enhancing Real-World Use Cases Objectives: To integrate meta-ros scarthgap layer onto AGL master branch for further releases. Bring-up the same and get it working on Pi500. Write a demo application showing interaction and communication between ROS libraries and AGL Application. Journey: The project covered many interconnected pieces, starting with integrating the Yocto layer meta-ros. During the community bonding period, my initial plan was to add the layer using bitbake-layer and build a default AGL target. I cloned meta-ros, added it as a BitBake layer in my build environment, and successfully compiled ros-core with the layer included. Hardware support became the next major task. I initially tested everything on QEMU x86_64, but our goal was to run on real hardware. I attempted a Raspberry Pi 5 target, which booted but showed frontend and UX glitches. With guidance from my mentors, we discovered the kernel source being used was outdated. To fix this, a patch adding the necessary DTS support was upstreamed, resolving the boot and display issues on the Pi 5. Up to this point, the ROS integration into AGL had been done manually. Since AGL’s build process relies on features enabled before compilation, we needed an AGL feature to standardize this. Because ROS is a development feature, it belonged in meta-agl-devel. I prepared and submitted a patchset introducing a new feature, meta-agl-ros2, which automatically includes the ROS 2 layers required to build ROS packages such as ros-core. Once the system built and booted reliably, I tested ROS publishers and subscribers directly on the hardware. Using standard ROS tutorials, message sending and receiving worked successfully within AGL. With the core integration proven, the next step was to develop a demo application. The idea was to go beyond terminal-based demonstrations and showcase ROS capabilities through an actual UI/UX. We spent time defining the scope of this application. To interface the application with ROS, we evaluated two Flutter libraries: ros-bridge and rcldart. We chose rcldart because it is more recent and actively developed. After contacting the maintainer, we received permission to use it. Communication between ROS and Flutter worked on my development system, but initially failed on hardware due to user-namespace differences. AGL applications run under a separate user, but after resolving this, we successfully tested custom publishers and subscribers on the device. With communication established, we needed a real-world use case. We chose to incorporate a camera. Early experiments with simulators didn’t yield much direction, but eventually we focused on object detection as a practical automotive feature. The idea was to detect objects in real time, enabling the car to warn the driver or even brake in critical scenarios. I implemented this in the application, and once we confirmed it was working, we expanded the concept further. With remaining time, we added a driver-monitoring feature that detects fatigue or drowsiness using camera input, allowing the system to alert the driver and trigger appropriate safety behavior. Results: Was successfully able to integrate meta-ros scarthgap on AGL master. Pi500 now boots successfully with raspberrypi5 target. The demo application is now extended from just subscriber/publisher sync, it also has camera detection running a model in background in ROS-end to be able to do face detection and person detection. Challenges: Initially Pi kernel upstream was quite old, which lacked dts support, so we had to add that in order to be able to boot-up Pi500. The ROS backend, which ran on the terminal, was not able to communicate with the application, we were able to debug this by finding out the userspace/namespace issues where the backend was running. A lot of glitches and components in the application, which got fixed along the way. Future Scope: Upstream the meta-ros layer and in-sync with further releases of AGL. Test the upstreamed layer with base use cases along with application. Maintain the application for higher upstream and for further hardware updates. Establish gRPC communication with AGL frontend, to have an end-to-end communication setup between AGL and ROS. Patch-sets: https://gerrit.automotivelinux.org/gerrit/c/AGL/meta-agl-devel/+/31201?usp=dashboard https://gerrit.automotivelinux.org/gerrit/c/AGL/meta-agl-devel/+/31175?usp=dashboard https://gerrit.automotivelinux.org/gerrit/c/AGL/meta-agl-devel/+/31265?usp=dashboard https://gerrit.automotivelinux.org/gerrit/c/AGL/meta-agl-devel/+/31266?usp=dashboard https://gerrit.automotivelinux.org/gerrit/c/AGL/AGL-repo/+/31178?usp=search https://gerrit.automotivelinux.org/gerrit/c/AGL/meta-agl-devel/+/31100?usp=search https://gerrit.automotivelinux.org/gerrit/c/apps/flutter-ros-demo/+/31275?usp=search https://gerrit.automotivelinux.org/gerrit/c/apps/flutter-ros-demo/+/31276?usp=search https://github.com/danascape/flutter-ros-demo Images: Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 22", "url": "/posts/GSoC-2025-week22/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-11-03 00:00:00 +0530", "content": "For this week, I pushed the patchsets in-sync with the current application, it compiles alright, and even the camera stream is working!!!!. The python library issue that I discussed is still present, so the face and person detection is not working, affected python libraries are fer, dlib, ultralytics. dlib and ultralytics seem to be bundled with meta-python-ai but they did not compile on my end. As for fer we do need to write a recipe for it. Images Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 21", "url": "/posts/GSoC-2025-week21/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-10-27 00:00:00 +0530", "content": "For this week, I have almost completed the application, and have tested it on native Linux, and am working on integrating it in yocto builds and recipes, I have camera stream working on pi but no the model yet. For this week, I am aiming to get the app to run fully on pi, and test out all the use cases. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 20", "url": "/posts/GSoC-2025-week20/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-10-20 00:00:00 +0530", "content": "Last week, I was able to implement model detection for the driver seat as well as the person seat, where each model works in different aspects, for driver seat, it is able to detect mood, fatigue, and for the front camera it is able to detect object distance and give warnings in both scenarios. I am compiling for pi500 to see how it goes on that end now, and can proceed finally to the full testing phase of the project. Additionally to this, I faced the critical bug scenario where the camera stream got stuck after 1-2 minutes, I have fixed that as well Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 19", "url": "/posts/GSoC-2025-week19/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-10-13 00:00:00 +0530", "content": "For last week, we were able to reach a final UX on the flutter application, and I am working on implementation for model switching, there are a lot of python dependencies being used, so I am re-working on cutting them up, like face-detection libraries usually use cuda, but we won’t have GPU on pi500, so I am working on trimming out those things. Additionally, the application works on pi500, I was able to get the camera stream going but no detection is working as of now, I am able to debug it and will spend time this week to continue. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 18", "url": "/posts/GSoC-2025-week18/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-10-06 00:00:00 +0530", "content": "For this week, I got the app UX changes, as suggested, and additionally I was able to get the app running on pi500, not the SDK yet, but the latest app, the camera stream requires a few python packages, which I need to ask about for installation. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 17", "url": "/posts/GSoC-2025-week17/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-09-29 00:00:00 +0530", "content": "For this week, I was able to refactor the app code and test it on the hardware itself, as we discussed at the Friday meeting regarding potential fixes for users. The closure was: Using ldconfig, along with logging into agl-driver user (su - agl-driver), and then communication with the application, which worked successfully. The refactored application code is complete, to make the code more readable (https://github.com/danascape/ros-flutter). Currently, I am trying to fix the app for camera stream, as soon as it works, I will be able to show it to you tonight if not then wednesday. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 16", "url": "/posts/GSoC-2025-week16/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-09-22 00:00:00 +0530", "content": "For this week, we got the application working in qemu environment, which required us to create ldconfig to ros2 libraries which exists in a separate path (/opt/ros/humble/lib), so the application is running alright with no issues so far, and even communicates with ros2. This was tested on both trout, and master branch with flutter workspace_automation env (v2.0 branch). mkdir /etc/ld.so.conf.d echo ‘/opt/ros/humble/lib’ | tee /etc/ld.so.conf.d/ros2.conf ldconfig The above were the steps I had to do, so for the same I pushed a patch on gerrit. It’s tested and works! Additionally to this, I was able to get the flutter-app recipe working and the app installed on AGL. The app runs but I am not able to get ros2 communication, maybe we need to debug that part. I will attach some screenshots for the same. Images As for ROS2, I was planning to talk to Rob regarding some references I can find which can help me in running ROS and a camera feed with object detection. https://www.youtube.com/watch?v=XqibXP4lwgA https://www.youtube.com/watch?v=CajBysjKKUk https://www.youtube.com/watch?v=GSZ6z3GQk7Y So mostly, my plan for this week is to fix the flutter application and make the code readable, since everything currently is in a single file, so I will try to separate out the utilities and Theming parts. This is the patch for application https://gerrit.automotivelinux.org/gerrit/c/AGL/meta-agl-devel/+/31175 and I would like to keep it on the gerrit as of now, until some base application part is complete where the code is readable for others. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: GSoC 2025 - Week 17 Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 15", "url": "/posts/GSoC-2025-week15/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-09-15 00:00:00 +0530", "content": "For this week, Me and Rob spent time on flutter workspace_automation, but there had been no updates on it so far, specifically run-agl-ivi-demo-flutter-qemu did not show up for us both. I was able to source the workspace_automation env to run the app on native linux but not the qemu environment. If I have qemu running on a window, it shows in flutter devices, but while trying flutter run -d desktop-homescreen, it throws me Failed to connect to the Wayland display. No such file or directory. I pushed a patch for the bitbake recipe of the app, but it needs LD_LIBRARY path set to ros2 libraries to compile. (export LD_LIBRARY_PATH=/opt/ros/humble/opt/rviz_ogre_vendor/lib:/opt/ros/humble/lib/x86_64-linux-gnu:/opt/ros/humble/lib) Link: https://gerrit.automotivelinux.org/gerrit/c/AGL/meta-agl-devel/+/31175 Additionally to this, we were able to resolve the issue and a demo flutter app is running on qemu. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: GSoC 2025 - Week 16 Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 14", "url": "/posts/GSoC-2025-week14/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-09-08 00:00:00 +0530", "content": "For this week, I was not able to do much, it was mostly study part, where I am trying to integrate a camera feed into the application as a test using rcldart, but I think rcldart lacks the support or is not working, ros-bridge worked, and I am still investigating on it. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: GSoC 2025 - Week 15 Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 13", "url": "/posts/GSoC-2025-week13/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-09-01 00:00:00 +0530", "content": "I was able to build and test patchsets for pi500 posted on gerrit. It boots up here now. As for flutter application, I do not have much to say regarding the UI, hopefully I will have something by this wednesday on our 1:1 Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: GSoC 2025 - Week 14 Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 12 - Mid-term 2", "url": "/posts/GSoC-2025-week12/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-08-21 00:00:00 +0530", "content": "This week, I had made some personal plans, so I spent most of my time in drafting the flutter application, and I succeeded in sending and receiving messages to ros2 running on my host via the flutter application. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: GSoC 2025 - Week 13 Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 11 - Mid-term 1", "url": "/posts/GSoC-2025-week11/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-08-17 00:00:00 +0530", "content": "This week was quite productive as my mentors walked me through the expectations for the mid-term evaluation and the key deliverables I needed to achieve. We also discussed possible directions for the application. After some brainstorming, we decided to move forward with building a Flutter-based application. At this stage, no specific GUI or feature set had been finalized, but the focus was on validating the basic workflow and ensuring communication between Flutter and ROS2. For Flutter integration with ROS2, we considered two main approaches: ros-bridge and rcldart. Since ros-bridge is relatively old and less actively maintained, we chose to experiment with rcldart, a more modern library recently introduced by Harun Kurt. This seemed like the right choice for long-term stability and better support with Dart/Flutter. To successfully clear the mid-term evaluation, I had three primary tasks: Boot the Raspberry Pi 500 with the AGL-ROS2 feature built in. Source and test ROS2 demo nodes to confirm everything was functional. Develop a basic Flutter application on my host Linux machine that communicates with ROS2 using the rcldart library. Overall, this milestone helped set the foundation for the second half of the project. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: Week 12: Mid-term Evaluation Ended Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 7,8,9,10 - Unexpected Hardware Issue", "url": "/posts/GSoC-2025-week7-8-9-10/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-08-10 00:00:00 +0530", "content": "WIP Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: Week 11: Mid-term Evaluation Started Journey: GSoC 2025 Journey" }, { "title": "Features and Future Scope in AOSP - Part 2", "url": "/posts/Features-and-Future-Scope-in-AOSP-Part-2/", "categories": "AOSP Tutorials", "tags": "AOSP, Android", "date": "2025-07-22 00:00:00 +0530", "content": "Something that started as a hobby of mine, to something that I do professionally in my daily life. Core Features Modular Architecture Dynamic partitions for seamless A/B updates (and easy rollback) across devices. Many System Components packaged as APEX modules. Mainline Modules allow Google to update system components via Play Store. GKI (Generic Kernel Image) standardizes kernel builds across various devices. Flexibility Modify or change SystemUI, Quick Settings, lockscreen, and much more. Runtime Resource Overlay framework for live theming and UI-tweaks all without rebuilding or reflashing. Add or remove default apps, permissions, or behavior. Extend frameworks/base to add your own services or APIs. Device Portability Supports bring-up on custom boards, embedded devices and even x86_64!. Includes HAL support via HIDL and AIDL. Tools like lunch, fastboot, and adb make flashing and testing straightforward. Bazel compatible manifests, adb with QUIC based wireless debugging and fastboot with seamless encryption and rollback‑protection flags makes testing straightforward. Debugging and Diagnostics Built-in support for logcat, dmesg, perfetto, and tracing tools. Debugging native crashes, init, and system_server flows is built into the platform. Support for selinux, audit logs, and boot logs for system hardening. Future Scope Once you’re fluent with AOSP internals, here’s what you can build, experiment with, or scale into: Custom OS Development Build custom AOSP forked operating systems (LineageOS, GrapheneOS). Tailor Android to fit specific products like educational tablets, kiosks, or PoS systems. Android on Non-Phone Devices Bring AOSP to Raspberry Pi, x86, or other SOMs. Work on Android TV, Wear OS, and Android Automotive OS. Use AOSP in robotics, industrial systems, and infotainment units. Spatial &amp; XR Platforms. Research and Innovation Explore Android security internals, SELinux policies, sandboxing (zygote). Modify the runtime (ART), resource management and more. Run performance benchmarks, memory leak detection, or boot time analysis. Productization Use AOSP as a base to build proprietary products with or without Google Mobile Services (GMS). Integrate it with hardware sensors, microcontrollers, and custom kernel drivers. Provide a flexible set of applications by choice. Careers AOSP experience opens doors in deep system-level engineering. Roles include: AOSP Engineer Work on full AOSP Stack, from frameworks to applications, and core libraries, maintaining source, updating patches, etc. Android Framework Developer Modify and extend platform behavior, build custom APIs, tweak system services. Android BSP / Board Bring-up Engineer Port Android to new hardware, write or integrate HALs, handle kernel-space issues. Embedded Android Developer Use Android as a base for headless, industrial, or minimal systems. Android Security Engineer Work on system hardening, SELinux, verified boot, encryption, and OTA update security. Kernel &amp; Driver Developer Customize the Linux kernel for Android, optimize power, write drivers (e.g., I2C, SPI, camera). OS Maintainer or Custom ROM Contributor Maintain forks of Android, add features, upstream patches, or support open devices. If you do not understand anything, that is alright, we will cover everything in this series. Stay tuned for Part 3. I am also planning to start with a Youtube channel: Stay tuned: Youtube Have questions or want to collaborate? Reach out to me on my email" }, { "title": "Getting Started with AOSP - Part 1", "url": "/posts/Getting-Started-with-AOSP-Part-1/", "categories": "AOSP Tutorials", "tags": "AOSP, Android, OS", "date": "2025-07-15 00:00:00 +0530", "content": "Today marks six years since I compiled my very first Android source build — it was Android 7.0 Nougat. Ever since then, I’ve been deeply involved in Android platform development, and there’s one question that keeps coming back to me, asked by thousands of people: “What does it actually take to get started with AOSP?” If you’re new to the Android Open Source Project, the sheer scale of it can feel overwhelming. I still get the occasional nightmare navigating through frameworks/base, HAL layers, and random blueprint files scattered across the tree. But here’s the thing: it is not scary. What Does It Really Take? This might be a biased take, but in my experience, the first step isn’t to understand the entire system, it’s just to get used to the build system and boot it somehow. Whether it be a custom smartphone, emulator, or any other embedded hardware. Start by learning how to: Download the source Build an image Flash it to a real device or emulator Once you’ve successfully compiled and flashed your first build, the next steps, tweaking features, debugging logs, upstreaming changes, or even building your own product will come in eventually. AOSP Can Be Anything Before I go deeper, I’ve decided not to throw everything into a single blog post. Instead, I’ll break this into a short series where I’ll cover: What you can do with AOSP (trust me, it’s a lot more than you can imagine) How to get started (step-by-step) Prerequisites (tools, machines, repositories) What is AOSP exactly? How long it takes to build, boot, and development Whether you’re an applications developer, a systems engineer, or just curious about the OS behind billions of devices, AOSP has something for you. For Today If you’re new and wondering where to begin, here’s my advice: Don’t aim to understand everything at once. Just try to build and boot a device first. Everything else — debugging, customization, mainlining, and contributing — will follow. Stay tuned for Part 2, where we’ll start with “What is AOSP, really?” and why it’s more than just a pile of code. I am also planning to start with a Youtube channel: Stay tuned: Youtube Have questions or want to collaborate? Reach out to me on my email" }, { "title": "GSoC 2025 - Week 6 - Final Build Patchset", "url": "/posts/GSoC-2025-week6/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-07-13 00:00:00 +0530", "content": "During this week, I had a productive call with my mentors, where we delved into the correct way of handling ROS package dependencies in Yocto builds. I gained a better understanding of the difference between two key directives: IMAGE_FEATURE and IMAGE_INSTALL. This distinction was crucial in finalizing my patchset and ensuring that the ROS packages are integrated correctly. I spent time exploring and understanding the differences between IMAGE_FEATURE and IMAGE_INSTALL in Yocto. This research ultimately helped me refine my final patch. For context, IMAGE_INSTALL is used to add packages directly to the target image, while IMAGE_FEATURE is used to enable certain image features that may include adding groups of packages or enabling particular functionalities. Knowing when to use each of these is important to properly manage dependencies and optimize the Yocto image. Updated agl-ros2.inc Configuration Here is the modification I made to the agl-ros2.inc file in meta-agl-ros2/conf/include/agl-ros2.inc: # Add minimal core ROS IMAGE_INSTALL:append = \" \\ ros-core \\ \" By appending ros-core to IMAGE_INSTALL, I ensured that the minimal set of ROS packages required for the system to function was included in the image. This change was important because it ensures that the system can run core ROS functionalities, which are vital for robotics and autonomous driving applications. You can view the final patchset on Gerrit Final Patchset on Gerrit. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: Week 7,8,9,10: Unexpected Hardware Issue Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 5 - Testing the patchset", "url": "/posts/GSoC-2025-week5/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-07-06 00:00:00 +0530", "content": "In the past week, due to a lack of time, I was unable to test the compilation of the patchset. This week, I focused on testing the patchset compilation. Build Configuration First, I verified that meta-ros2 and meta-ros2-humble were getting picked up, and new environment variables appeared in my build configuration. Build Configuration: BB_VERSION = \"2.8.0\" BUILD_SYS = \"x86_64-linux\" NATIVELSBSTRING = \"universal\" TARGET_SYS = \"aarch64-agl-linux\" MACHINE = \"raspberrypi5\" DISTRO = \"poky-agl\" DISTRO_VERSION = \"19.0.2\" TUNE_FEATURES = \"aarch64\" TARGET_FPU = \"\" DISTRO_NAME = \"Automotive Grade Linux\" ROS_DISTRO = \"humble\" ROS_VERSION = \"2\" ROS_PYTHON_VERSION = \"3\" meta-raspberrypi = \"HEAD:3b27c95c163a042f8056066ec3d27edfcc42da7f\" meta-lts-mixins_u-boot = \"HEAD:66ceeebd047d7fdfc8668b300319a76da8ae257d\" meta-selinux = \"HEAD:c4b059262089b74c8fbf8dd5fdf5fd7bc1deeddc\" meta-pipewire meta-agl-kuksa-val meta-agl-flutter = \"HEAD:c6064451dacccfb0d8c89fe3e5a606a98458b5ad\" meta-flutter meta-flutter-apps = \"HEAD:f12d340d5ac0bb5519e0bdb3d750c805501af308\" meta-app-framework = \"HEAD:c6064451dacccfb0d8c89fe3e5a606a98458b5ad\" meta-agl-ros2 = \"HEAD:f9cd7a20cfc95004628c7e316a1fc55b124e6358\" meta-ros-common meta-ros2 meta-ros2-humble = \"scarthgap:ebef2e1409307c7eb622d2fe85467f12cead9d89\" meta-agl-demo = \"HEAD:c05b06465e757a1c1cd57d66e244b5d9b55c44e9\" meta-networking meta-python meta-filesystems meta-multimedia = \"HEAD:e92d0173a80ea7592c866618ef5293203c50544c\" meta-clang = \"HEAD:eaa08939eaec9f620b14742ff3ac568553683034\" meta-qt6 = \"HEAD:57fef415fcde6c3d70a028f42f318f455633dc97\" meta-oe = \"HEAD:e92d0173a80ea7592c866618ef5293203c50544c\" meta-agl-core meta-agl-bsp = \"HEAD:c6064451dacccfb0d8c89fe3e5a606a98458b5ad\" meta meta-poky = \"HEAD:bab0f9f62af9af580744948dd3240f648a99879a\" Compilation I then proceeded to compile by running the following command: source meta-agl/scripts/aglsetup.sh -f -m raspberrypi5 -b raspberrypi5 agl-flutter agl-devel agl-demo agl-ros2 bitbake agl-ivi-demo-flutter The build completed successfully, and I flashed the build onto the device. It booted up without issues. Issue with ROS Dependencies However, I realized that no ROS dependencies were built or installed into the root filesystem. Upon further investigation, I discovered that I needed to add or create an additional file to specify which dependencies should be compiled when the layer is included. My mentor pointed out this mistake, which I had overlooked in the rush. But I did not realise and checked that there was no ros dependency that got built and installed into the rootfs. On further investigation I found out that there is an additional file we need to add or create, in order to add a set of dependency to be compiled when the layer is included, my mentor pointed this out which was a hurried mistake on my end. Solution To resolve the issue, I created the meta-agl-ros2/conf/include/agl-ros2.inc inside meta-agl-devel layer file with the following content: IMAGE_FEATURES =+ \" ros-core\" I included this file with an external layer. However, I wasn’t able to figure out the proper way to include this dependency. For now, I chose ros-core as the most basic dependency I could think of. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: Week 6: Comprehensive Testing and Validation Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 4 - First Patch to Gerrit", "url": "/posts/GSoC-2025-week4/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-06-29 00:00:00 +0530", "content": "In the previous week, I was confident that I could begin integrating the ROS feature into AGL environment. We had already decided that we needed to work with ROS2 Humble. This week, my focus shifted toward preparing the integration process and pushing my first patch to the community. To kickstart the integration of ROS2 into AGL, I began by exploring how other features were integrated into the system. Since we’re in the development phase and ROS2 is not an active feature in AGL yet, I opted to add the necessary components inside the meta-agl-devel layer. Here are the key steps I followed for integration: Creating a layer template for agl-ros2 feature (meta-agl-ros2/conf/layer.conf) # We have a conf and classes directory, add to BBPATH BBPATH =. \"${LAYERDIR}:\" # We have recipes-* directories, add to BBFILES BBFILES += \"${LAYERDIR}/recipes-*/*/*.bb \\ ${LAYERDIR}/recipes-*/*/*.bbappend\" BBFILE_COLLECTIONS += \"aglros2\" BBFILE_PATTERN_aglros2 = \"^${LAYERDIR}/\" BBFILE_PRIORITY_aglros2 = \"70\" LAYERSERIES_COMPAT_aglros2 = \"scarthgap\" Creating a bblayer to add layer dependencies when called (templates/feature/agl-ros2/50_bblayers.conf.inc) BBLAYERS =+ \" \\ ${METADIR}/meta-agl-devel/meta-agl-ros2 \\ ${METADIR}/external/meta-ros/meta-ros-common \\ ${METADIR}/external/meta-ros/meta-ros2 \\ ${METADIR}/external/meta-ros/meta-ros2-humble \\ \" A feature description or README (templates/feature/agl-ros2/README_feature_agl-ros2.md) --- description: Feature agl-ros2 authors: Saalim Quadri &lt;saalimquadri2@gmail.com&gt; --- ### Feature agl-ros2 Patchset Submission Once the integration steps were completed, I proceeded to prepare my first patchset. I pushed the patch to Gerrit for review, where it can be further evaluated by the community and maintainers. You can find the patchset here: My first patchset is First Patchset to Gerrit Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: Week 5: Expanding ROS Support in AGL Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 3 - Starting to Code", "url": "/posts/GSoC-2025-week3/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-06-22 00:00:00 +0530", "content": "By this point, I had both AGL and ROS building successfully on their own. That meant there shouldn’t be any major issues when compiling them together—so it was finally time to start integrating them. During my face-to-face call with my mentors, I asked how I should approach the integration. My first step was to clone the meta-ros scarthgap branch, initialize the AGL environment, and try compiling a ROS-specific package. I cloned the meta-ros layer into external/meta-ros inside the AGL home directory: git clone -b scarthgap https://github.com/ros/meta-ros external/meta-ros Checking the AGL directory structure: ➜ agl tree -L 1 . ├── bsp ├── external ├── meta-agl ├── meta-agl-demo ├── meta-agl-devel ├── qemux86-64 ├── raspberrypi4 └── raspberrypi5 And inside the external folder: ➜ agl tree -L 1 external external ├── meta-clang ├── meta-codechecker ├── meta-flutter ├── meta-openembedded ├── meta-python-ai ├── meta-qt6 ├── meta-ros ├── meta-security ├── meta-selinux ├── meta-spdxscanner ├── meta-tensorflow ├── meta-virtualization ├── poky └── workspace-automation I then initialized the AGL environment and built ros-core: source meta-agl/scripts/aglsetup.sh -f -m raspberrypi4 -b raspberrypi4 agl-flutter agl-devel agl-demo bitbake ros-core It took a while. I even had to do a clean build since I hadn’t set up an sstate cache or a local downloads server—meaning everything had to be fetched and built from scratch. Eventually, the build completed successfully. It took some time, I did a clean build again, at this point I did not setup sstate cache and downloads server to my local, so It downloaded everything again and compiled, eventually I succeeded in this, and now just had to figure out the template Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: Week 4: First Patch to Gerrit Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 1,2 - Setting Up the Build", "url": "/posts/GSoC-2025-week1-2/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-06-15 00:00:00 +0530", "content": "Environment Setup Having experience working with the Android Open Source Project and the Linux Kernel proved beneficial as it made managing larger sources more straightforward. Surfing and downloading sources was easier for me, though I understand it might be different for others. To streamline the process, I procured a powerful PC, knowing that AOSP can take over two hours to compile. Patience, honed through previous experience, helped me tackle these challenges with ease. Technical Environment Setup Development System Specifications My primary development environment: Hardware: Ryzen 7 7700X, 32GB RAM, 4TB SSD Primary OS: Ubuntu LTS 22.04 Secondary OS: Windows 11 AGL and meta-ros version Selection After extensive discussion with my mentors, we decided on: AGL Release: Super Salmon (latest stable at project start) meta-ros Layer: Scarthgap branch (compatible with AGL’s OE version) Target ROS Distribution: ROS2 Humble This combination provides the best balance of stability and feature completeness for automotive applications. Building Initial Images Setting up the build environment was more complex than I anticipitated: AGL Setup mkdir agl-gsoc &amp;&amp; cd agl-gsoc repo init -u https://gerrit.automotivelinux.org/gerrit/AGL/AGL-repo repo sync source meta-agl/scripts/aglsetup.sh -f -m raspberrypi4 -b raspberrypi4 agl-flutter agl-devel agl-demo bitbake agl-ivi-demo-flutter You can find more details on dependencies here. ROS Layer build mkdir ros-gsoc &amp;&amp; cd ros-gsoc git clone -b build https://github.com/ros/meta-ros mkdir ros-out KAS_WORK_DIR=ros-out kas build meta-ros/kas/oeros-kirkstone-humble-raspberrypi4-64.yml For more instructions, check the kas README. I initially chose to build both the AGL and ROS layers independently on my machine. However, this process took much longer than expected. While I had many of the necessary dependencies already installed, I encountered numerous errors. These issues were particularly perplexing to my mentors, as they seemed related to conflicting dependencies from my previous Android cuttlefish setup, which caused errors I hadn’t anticipated. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: Week 3: First Steps - Starting to Code Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Week 0 - Community Bonding", "url": "/posts/GSoC-2025-week0-community-bonding/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-06-01 00:00:00 +0530", "content": "Welcome to my GSoC Journey This week marks the beginning of my GSoC 2025 adventure with The Linux Foundation, working on integrating meta-ros with Automotive Grade Linux (AGL). The community bonding period is crucial - it’s when we transition to a active contributor in an open-source community. Meeting My Mentors Jan-Simon Möller and Walt Miner from AGL and Rob Woolley from the ROS community have been incredibly welcoming and patient with all my initial questions. Our first video call was enlightening, they provided context about AGL’s current state, the challenges with ROS integration, and realistic expectations for the project scope. Key insights from mentor discussions: AGL’s current focus and versioning ROS’s and meta-ros establishments and versionings The importance of development through Gerrit Hardware considerations for automotive embedded systems Joining the AGL Community Weekly Developer Calls I started attending the AGL Weekly Developer Calls, these happen every Thursday, we got introduced with the community, and the maintainers and listened to the goldmine of information from everyone. The community is very welcome the newcomers. You can find the calendar here, it is open to all! The Weekly Developer Call is for all developers engaging or to be engaged with the AGL community, additional to this, we had a weekly GSoC catch-ups with our mentors and other students with their mentors. Here I got to learn about other candidates, their approaches and much more. For me, I asked for a 2 day face-to-face call with my mentors, which they agreed to without any issue, there we discussed issues and analysed about what all we can do in scope of our project. Community Engagement Highlights Mostly spent listening and understanding the terminology Asked a few questions about builds, ROS, etc. Project Scope One important lesson during the community bonding period was that, after being selected for GSoC, the proposal you submitted can evolve significantly. During this period, we explored many aspects of the project before reaching a solid starting point for integrating the two technologies. There were numerous debates, but in the end, we found a common ground. The community bonding period has been invaluable for understanding not just the technical challenges, but the collaborative processes that make open-source projects successful. I’m excited to start the hands-on development work! Week 0 Summary: Foundation laid, community connections established, technical environment prepared. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. I am planning to include examples, code, etc, such that anyone can follow along with it. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next: Week 1,2: Hardware Setup and Initial Build Environment Journey: GSoC 2025 Journey" }, { "title": "GSoC 2025 - Journey", "url": "/posts/GSoC-2025-Journey/", "categories": "GSoC", "tags": "Yocto, AGL, RaspberryPI, GSoC, LFX, LF", "date": "2025-05-08 00:00:00 +0530", "content": "Automotive Grade Linux (AGL) and ROS Integration What is Google Summer of Code? “Google Summer of Code is a global, online program focused on bringing new contributors into open source software development. GSoC Contributors work with an open source organization on a 12+ week programming project under the guidance of mentors.” Project Background This summer, I’m working on Google Summer of Code 2025 with the Linux Foundation, focusing on integrating Robot Operating System (ROS) with Automotive Grade Linux (AGL) through the meta-ros layer. What is Yocto? The Yocto Project is an open-source collaboration focused on creating custom Linux distributions for embedded systems. It provides tools and a framework to build tailored Linux images for various hardware architectures. Essentially, it allows developers to create a unique Linux OS for embedded devices rather than relying on pre-built distributions. What is Automotive Grade Linux (AGL)? Automotive Grade Linux is an open-source collaborative project that brings together automakers, suppliers, and technology companies to develop a unified software platform for connected cars. Think of it as Android for cars, but built from the ground up for automotive requirements. AGL Architecture Overview AGL’s architecture consists of five distinct layers: App/HMI Layer: Applications and human-machine interfaces Application Framework: APIs for creating and managing automotive applications Services Layer: User space services accessible to all applications Operating System Layer: Linux kernel, drivers, and standard utilities Hardware Layer: Underlying automotive hardware platform The meta-ros Layer: Bridging ROS and Embedded Systems Meta-ros is a series of OpenEmbedded layers that add ROS support to embedded Linux distributions built with the Yocto Project. It’s essentially the bridge that allows us to run ROS on custom automotive hardware. Key Components meta-ros-common: Common recipes for all ROS distributions Distribution-specific layers: Separate layers for ROS Humble, Iron, etc. This architecture enables developers to create custom, Linux-based automotive systems that leverage both ROS capabilities and Yocto’s flexibility for embedded deployments. Why ROS in Automotive Matters The integration of ROS with automotive systems offers several compelling advantages: Modular Architecture Individual components can be developed and tested independently Enables rapid prototyping and algorithm comparison Supports different sensor configurations without architectural changes Facilitates collaborative development across teams Advanced Sensor Fusion Unified timestamp management across different sensors Real-time fusion of LIDAR, camera, radar, and GPS data Extended Kalman Filter implementations for data association Enhanced perception through sensor complementarity Rich Ecosystem Vast library of pre-existing packages and algorithms Active community support and continuous development Integration with simulation environments (Gazebo, CARLA) Cost-effective compared to proprietary solutions Current Progress and Next Steps The project is progressing well across multiple fronts. I’ve successfully built custom AGL images with integrated ROS2 support and created automotive-specific sensor drivers that work within the AGL environment. Completed Milestones Successfully integrated meta-ros with AGL Scarthgap Created custom recipes for automotive sensor packages Established cross-compilation pipeline for ARM-based automotive hardware Upcoming Work Developed initial architecture documentation Performance benchmarking and optimization Integration with automotive simulation environments Creation of reference applications demonstrating key use cases Documentation and community outreach GSoC 2025 Weekly Blog Series This project spans 22 weeks, and I’ll be documenting the journey through a series of weekly blog posts. Each week will detail the technical challenges, solutions, and learnings from integrating meta-ros with Automotive Grade Linux. Blog Series Structure Week 0: Community Bonding Week 1+2: Setting Up the Build Week 3: First Steps - Starting to Code Week 4: First Patch to Gerrit Week 5: Expanding ROS Support in AGL Week 6: Comprehensive Testing and Validation Week 7,8,9,10: Unexpected Hardware Issue Week 11: Mid-term Evaluation Started Week 12: Mid-term Evaluation Ended Week 13: Weekly Updates Week 14: Weekly Updates Week 15: Weekly Updates Week 16: Weekly Updates Week 17: Weekly Updates Post-GSoC Continuation: Weeks 10-22: Demo Application and Long-term Maintenance and Integration Current Status As of August 2025, I’ve completed 10 weeks of the project with significant milestones achieved: ✅ Meta-ros successfully integrated with AGL ✅ Functional demo application with bidirectional ROS-AGL communication ✅ Patchsets contributed to AGL Gerrit Currently working on Week 10: Real-time sensor data integration and advanced visualization features. Get Involved Stay tuned for the detailed technical deep-dives in the upcoming blog posts. Each will include practical examples, code samples, and hands-on tutorials that you can follow along with. Have questions about automotive software development or want to collaborate on open-source automotive projects? Reach out to me on my email Next post in series: GSoC 2025 Week 0: Community Bonding" }, { "title": "Using AOSP Kernel Build", "url": "/posts/Using-AOSP-Kernel-Build/", "categories": "Linux Kernel", "tags": "Linux, Kernel, Linux Kernel, Bazel, Make", "date": "2024-08-14 00:00:00 +0530", "content": "In my previous blog, we talked about how to cross-compile Android Kernel, but there is another official way that ODMs use to compile their kernel sources, via AOSP Kernel Build. This blog covers 2 aspects: Compile using Legacy AOSP Kernel Build Add support for bazel Kernel Build Bazel is an open-source build and test tool from Google. It supports a wide range of programming languages and platforms, and it’s known for its speed and ability to handle large codebases efficiently. Bazel uses a build system that allows for incremental builds, parallel execution, and sophisticated dependency analysis, making it a popular choice for complex projects like AOSP. AOSP Kernel Build System is the system used to compile the Linux kernel for Android devices. It includes the necessary build scripts, configurations, and toolchains needed to build the kernel from source. The build system is designed to work with AOSP’s infrastructure and often uses tools like make and ninja for the actual compilation process. In recent years, Bazel has been adopted by some parts of the AOSP for building specific components, but the traditional kernel build process has largely remained based on make. However, there’s ongoing work to integrate Bazel more deeply into AOSP’s build system to leverage its advantages, like better dependency tracking and faster builds. Prerequisites Before diving into the AOSP Kernel Build process, certain requirements need to be met: Basic File and Text Editing: Familiarity with editing and writing files and text in a Linux environment. Linux Command Line: Comfort with using the Linux command line interface. Basic Git Knowledge: Understanding basic Git commands and workflows. Repo Sync and Manifests: Familiarity with syncing repositories using repo sync and working with manifest files. Establishing a Build Environment If you’ve already set up a Linux build environment, you can skip this step. Otherwise, follow these instructions to get started: Setup Script: Run the following command on your Linux machine with elevated permissions to set up the necessary environment: curl https://raw.githubusercontent.com/akhilnarang/scripts/master/setup/android_build_env.sh | sudo sh This script will install all the required dependencies for building the AOSP Kernel. Setup Kernel Manifest To build the AOSP kernel, you need to set up a kernel manifest, which will allow you to sync the required scripts and compiler. To do that, head to here and sync the required manifest. I would recommend using common- and your kernel version branch. repo init -u https://android.googlesource.com/kernel/manifest/ -b common-android-4.19-stable --depth 1 Depth Sync: The --depth 1 option is used to perform a shallow sync, which is faster because it doesn’t download the entire history. This is ideal if you don’t need to develop on the build system itself. If you do plan to develop further, you can perform a full sync by omitting the --depth 1 option. After a sync, you can view the structure of the source tree: ~/kernel/sp# tree -L 1 . ├── build ├── common ├── common-modules ├── hikey-modules ├── kernel ├── prebuilts ├── prebuilts-master └── tools 8 directories, 0 files Inside the common/ directory, you’ll find the Android Common Kernel source, which we won’t need for our purposes. You can replace this with your own kernel source. Replace common kernel directory with your kernel source with version V4.19. Setup Build Config The AOSP Kernel Build system relies on a BUILD_CONFIG variable, which configures the build environment, including kernel configuration and compiler flags. Create a file named build.config.&lt;device-name&gt; Write basic configuration, like your kernel build directory, which is set to common by default, then required defconfig KERNEL_DIR=common . ${ROOT_DIR}/${KERNEL_DIR}/build.config.billie.common.clang Create a file named build.config.&lt;device-name&gt;.common.clang Specify compiler settings, I am differing the names with my perspective, these can be anything. . ${ROOT_DIR}/${KERNEL_DIR}/build.config.billie.common CC=clang LD=ld.lld CLANG_TRIPLE=aarch64-linux-gnu- Create a file named build.config.billie.common To call the common configuration and define additional settings. ```Makefile . ${ROOT_DIR}/${KERNEL_DIR}/build.config.common BUILD_INITRAMFS=1 DEFCONFIG=vendor/billie-perf_defconfig Here, the `build.config.common` file, which is already present in the kernel directory, contains the remaining configuration. You can find the full reference commit [here][akb-billie-configs]. Fill in the remaining configuration details as per your needs. Depending on your kernel version, you can fully utilize the Build System to start the kernel build process and package the distribution into a boot image. ## Starting the build Once you have set up your build environment and configured the necessary build files, you can begin compiling the kernel. The AOSP kernel build process uses the `build/build.sh` script, and the build is controlled by the `BUILD_CONFIG` environment variable. ### Step-by-Step Guide to Starting the Build 1. **Setting the `BUILD_CONFIG` Variable:** * The `BUILD_CONFIG` variable specifies the path to your configuration file, which contains all the necessary settings for your build, including kernel directories, compiler settings, and other environment variables. * You can set the `BUILD_CONFIG` variable by running the following command in your terminal: ```Makefile export BUILD_CONFIG=common/build.config.&lt;device-name&gt; Replace &lt;device-name&gt; with the actual name of your device, or the name you used in your configuration files. Running the Build Script: With the BUILD_CONFIG variable set, you can now trigger the build process by executing the build/build.sh script. The -j option is used to specify the number of jobs to run simultaneously, which can significantly speed up the build process on multi-core systems. For example, to run the build with 14 parallel jobs, you can use: build/build.sh -j14 This command will start the kernel compilation process using the configuration you specified in the BUILD_CONFIG file. Monitoring the Build Process: As the build progresses, you’ll see various messages in the terminal, including the compilation of individual source files and linking of the final kernel image. The time it takes to complete the build will depend on your system’s hardware and the complexity of the kernel being built. On a modern multi-core system, the build might take anywhere from several minutes to an hour or more. Successful Compilation: If everything is configured correctly, the build should complete without errors, and the kernel image will be successfully compiled. Customizing build.config.common It’s important to note that the build.config.common file can vary depending on the kernel sources you’re working with. This file contains common settings that are shared across different configurations, such as the default compiler, build options, and any additional environment variables. Modifying build.config.common: You may need to customize this file to match the specific requirements of your kernel version or device. For instance, the file might include different compiler flags or specify a different defconfig file depending on the target device or kernel features you’re working with. Part 2: Converting to BUILD.bazel Initial Impressions When I first started experimenting with Bazel for kernel builds, my initial reaction was that it seemed overly complex. However, after spending more time with it and testing it across different kernel versions, I came to a conclusion that it is still in development. It’s important to note that Bazel support in kernel builds is still evolving. Currently, only the latest Pixel kernels (starting from Pixel 8 and kernel version 5.15+) fully support Bazel builds. If you’re working with older kernel versions or downstream kernels, you’ll face challenges as Bazel support is either incomplete or entirely missing. If you’re interested in trying Bazel for your custom kernel builds, I strongly recommend starting with kernel version 5.15 or newer. These versions are better supported, and you’ll encounter fewer issues. For those working with older or downstream versions (like the 4.19 kernel mentioned earlier), be prepared for a more challenging experience. You’ll likely need to make significant modifications to get Bazel working correctly. In this section, I’ll share my experience porting a downstream version 4.19 kernel to Bazel, highlighting the challenges I encountered. Understanding the Basic Bazel File Structure At the heart of the Bazel build system is the BUILD.bazel file. This file is used to define build rules, targets, and dependencies for your project. In the context of a kernel build, the BUILD.bazel file acts as a blueprint, specifying how the kernel and its components should be compiled and linked. Prebuilt Bazel Definitons The top-level BUILD.bazel file typically inherits prebuilt Bazel definitions. These definitions are found in the build/bazel/kleaf directory within the AOSP kernel manifest source tree. The kleaf directory contains essential Bazel macros and rules specifically designed for kernel builds. You can load these definitions into your BUILD.bazel file and override them to customize the build process. This allows you to: Declare New Build Targets: Define specific targets for different kernel components or modules. Use External Kernel Modules: Integrate external modules that may not be part of the main kernel source. Customize GKI Configs: Manage Generic Kernel Image (GKI) configurations, which are crucial for Android devices. Create Boot Images: Define how the boot image is generated, including the integration of the compiled kernel and ramdisk. Example of a basic BUILD.bazel file Here’s a simplified example of what a basic BUILD.bazel file might look like: package( default_visibility = [ \"//visibility:public\", ], ) load(\"@bazel_skylib//rules:common_settings.bzl\", \"string_flag\") load(\"//build/bazel_common_rules/dist:dist.bzl\", \"copy_to_dist_dir\") load( \"//build/kernel/kleaf:kernel.bzl\", \"kernel_build\", ) load(\"@kernel_toolchain_info//:dict.bzl\", \"BRANCH\", \"CLANG_VERSION\") kernel_build( name = \"kernel_billie\", srcs = glob( [\"**\"], exclude = [ \"**/BUILD.bazel\", \"**/*.bzl\", \".git/**\", ], ), outs = [ \"Image\", \"System.map\", \"modules.builtin\", \"modules.builtin.modinfo\", \"vmlinux\", \"vmlinux.symvers\", ], build_config = \"build.config.billie\", ) Below is a basic BUILD.bazel file that I created to compile the kernel source discussed earlier, utilizing the kleaf framework provided by Bazel. This file is tailored to work with the specific kernel version and configuration we’ve been discussing. Rather than me explaining each and every bit of line, I’ll prefer going through bazel documentation to checkout, the changes from build.config towards BUILD.bazel here. Sync Kernel Manifest To ensure you’re working with the latest tools and scripts required for kernel compilation, I recommend syncing the common-android-mainline branch from the kernel/refs+. This branch is kept up-to-date with the latest changes and includes upstreamed scripts that are essential for a smooth build process. repo init -u https://android.googlesource.com/kernel/manifest -b common-android-mainline --depth 1 This branch is particularly advantageous because it includes: Latest Compilation Tools: Ensures you have the most recent versions of tools required for kernel builds. Upstreamed Scripts: Contains updated scripts that streamline various stages of the build process, from kernel configuration to image generation. Boot Image Generation: Provides tools necessary for creating boot images directly from the compiled kernel. Kernel ABI and Symbol Generation: Includes utilities for generating ABI symbols, which are critical for ensuring compatibility with the Android ecosystem. Distribution and Artifacts: Facilitates the creation of distribution artifacts, making it easier to package and deploy your kernel. After Sync, the new source tree should look like: ~/kernel/mainline# tree -L 1 . ├── build ├── common ├── common-modules ├── external ├── kernel ├── MODULE.bazel -&gt; build/kernel/kleaf/bzlmod/bazel.MODULE.bazel ├── prebuilts ├── test └── tools 8 directories, 1 file Once the source tree is set up, the next step is to replace the common directory with your custom kernel source. After doing this, you can proceed to write or modify the BUILD.bazel file to tailor the build process to your specific kernel.To compile the kernel target Start the build To initiate the build process for your custom kernel using Bazel: tools/bazel build //common:kernel_billie tools/bazel: This specifies the path to the Bazel executable or script that we will use to run the build. build: This Bazel command triggers the build process for the specified target. //common:kernel_billie: This is the Bazel target you are building. In this case, kernel_billie is the target defined in the BUILD.bazel file located in the common directory. This will start the compilation process. Porting a Downstream Version 4.19 Kernel to Bazel When I attempted to port a 4.19 downstream kernel to Bazel, I encountered several challenges: Missing Bazel Support: The kernel version didn’t natively support Bazel, so I had to manually create BUILD.bazel files for various components. Toolchain Compatibility: Ensuring that the toolchain definitions in the Bazel files matched the kernel’s requirements was tricky, especially when dealing with older versions of Clang or GCC. Dependency Management: Some external kernel modules and dependencies required additional customization to work with Bazel. Successful Compilation If everything is configured correctly, the build should complete without errors, and the kernel image will be successfully compiled. For those interested in diving deeper, I encourage you to explore the official Bazel documentation, which provides extensive resources on setting up and customizing Bazel builds. Ill put everything in references, or you can mail me to ask specific things. Tips &amp; Tricks: When setting up your own kernel build definitions and Bazel configurations, it’s beneficial to refer to the Pixel kernel manifest repositories and their associated build.config.pixel files. These resources are maintained by Google and provide a robust base for writing your custom build definitions and Bazel declarations. References: Kernel Manifest android-msm-billie-4.19 linux-sunny android-msm-ginkgo-4.14 Kernel Trees: linux-oneplus-billie kernel_xiaomi_ginkgo AOSP Kernel Build Build with Bazel Let me know your new experiences at my email." }, { "title": "Cross-Compiling Android Linux Kernel", "url": "/posts/Cross-Compiling-Android-Linux-Kernel/", "categories": "Linux Kernel", "tags": "Linux, Kernel, Linux Kernel", "date": "2024-07-24 00:00:00 +0530", "content": "When you use your Android device, you’re interacting with a complex system that relies heavily on the Linux kernel at its core. But what exactly is the Android Linux kernel, and how does it work? What is Linux Kernel? The Linux kernel is the core component of the Linux operating system, responsible for managing hardware resources (such as the CPU, memory, and devices), providing essential services to software applications, and ensuring secure and efficient operation of the entire system. What is the Android Linux Kernel? The Android Linux kernel is a version of the Linux kernel, based on Long Term Stable (LTS). At Google, LTS kernels are combined with Android-specific patches to form what are known as Android Common Kernels (ACKs). For the tutorial, I’m using Ubuntu 22.04 running inside a 8 Core, 16GB RAM machine. However, the steps should be the same independent of whether you’re using a virtual machine, running a different version of Linux, etc. Prerequisites Certain requirements are to be met before compiling the Linux Kernel. Ability to understand basic editing/writing of files and text, familiarity with the linux command line interface and some basic git knowledge. Establishing a Build Environment To build the Kernel, a Linux build machine is recommended, with some minimal specifications as long as it can configure make. To setup the Linux build environment , you can directly run this command on your linux machine with elevated permissions: curl https://raw.githubusercontent.com/akhilnarang/scripts/master/setup/android_build_env.sh | sudo sh 1. Set up the cross-compiling toolchain In order to compile source code into machine code that is not native to the build machine, a cross-compiler has to be used. $ git clone --depth=1 -b 11.x https://gitlab.com/stormbreaker-project/google-clang clang-11 $ git clone --depth=1 https://github.com/stormbreaker-project/aarch64-linux-android-4.9 gcc64 $ git clone --depth=1 https://github.com/stormbreaker-project/arm-linux-androideabi-4.9 gcc32 $ export PATH=$PATH:$PWD/clang-11/bin:$PWD/gcc64/bin:$PWD/gcc32/bin 2. Download the Source Code We are going to pickup a source that is already being worked on, you can take sources from ODM pages of various android devices that are launched. Google: https://android.googlesource.com/kernel/msm/+refs Xiaomi: https://github.com/MiCode/Xiaomi_Kernel_OpenSource OnePlus: https://github.com/oneplusoss Nothing: https://github.com/nothingoss Realme: https://github.com/realme-kernel-opensource Samsung: https://opensource.samsung.com/main So after hovering around, download your favorite source code in a directory $ git clone https://github.com/stormbreaker-project/linux-oneplus-billie We generally do not require the commit history, so to speed up the process, use the --depth argument to only clone the most recent version of all the files: $ git clone --depth=1 https://github.com/stormbreaker-project/linux-oneplus-billie This can take a few minutes. 3. Configure the Build Next, we need to configure the kernel build. The easiest option is to just build with the default configuration cd linux-oneplus-billie make O=out ARCH=arm64 vendor/billie-perf_defconfig Alternatively, we can also copy the configuration from an existing build. $ adb pull /proc/config.gz $ gunzip -c config.gz &gt; out/.config This will use adb to establish a connection with the device and copy the configuration from the default location at /proc/config.gz. This is a virtual location provided by a kernel module. 4. Build the Kernel Now that everything is configured, we can start the build process. Run the following command to build the kernel image. Modify the -j parameter to approximately correspond with the number of CPU cores your host machine has. Higher values will lead to fast build times. make -j14 O=out ARCH=arm64 CROSS_COMPILE=aarch64-linux-android- CROSS_COMPILE_ARM32=arm-linux-androideabi- CC=clang CLANG_TRIPLE=aarch64-linux-gnu- The build machine will now take a while to compile the kernel. This might vary as per machine configuration, it took me about 10-15 minutes until the build was finished. 5. Booting the Kernel All that’s left now is to flash the kernel Image. There are various methods to install a kernel image on android devices. AnyKernel3: https://github.com/osm0sis/AnyKernel3 Android mkboot: https://android.googlesource.com/platform/system/tools/mkbootimg/ Live Boot Here we will use LiveBoot, since the other methods requires many more device-side features, so let’s keep that aside. To Live Boot the Kernel image, reboot your device to bootloader. fastboot boot out/arch/arm64/boot/Image.gz The device will reboot automatically, and run the freshly installed kernel. Note: Flash methods vary for each device and manufacturer, so I used a generic method in the process which MIGHT NOT work for some devices. For AnyKernel3 to work, one needs to make device specific recovery changes, like defining boot partition path and device spec name, for it to work. Not all Flash methods are mentioned here. Refer to your ODM guides on how to install the kernel image. Let me know your new experiences at my email." }, { "title": "LFX Mentorship for Linux Kernel Bug Fixing", "url": "/posts/LFX-Mentorship-for-Linux-Kernel-Bug-Fixing/", "categories": "Linux Kernel", "tags": "Linux, Kernel, Linux Kernel, LFX", "date": "2023-07-27 00:00:00 +0530", "content": "Learn more about the Linux Foundation Mentorship programme, and hear from someone who went through the process. What is LFX mentorship? “The Linux Foundation Mentorship Program is designed to help developers — many of whom are first-time open source contributors — with necessary skills and resources to learn, experiment, and contribute effectively to open source communities. By participating in a mentorship program, mentees have the opportunity to learn from experienced open source contributors as a segue to get internship and job opportunities upon graduation.” Please have a look at the Mentorship guide to learn how to participate in LFX Mentorship programs: lfx.linuxfoundation.org/mentorship/guide Linux Kernel Mentorship Programme (LKMP) Linux Foundation mentorship programs offer unique opportunities for aspiring developers to contribute to the Linux Kernel, which is the core of the Linux Operating System. The Linux Kernel is an open-source project that continuously evolves through the collective effort of thousands of developers worldwide. LKMP is one of the programmes run by the Linux Foundation where we get to learn about subsytems inside Linux Kernel and how to contribute to them. Under the mentorship, we got hands-on experience in debugging and resolving issues within the kernel. The program follows a structured approach to ensure participants gain valuable hands-on experience and become proficient in kernel development. About Device Tree Bindings The Device Trees were originally created by Open Firmware as part of the communication method for passing data from Open Firmware to a client program. An operating system used the Device tree to discover the topology of the hardware at runtime, and thereby support a majority of available hardware without hard coded information. The Device Tree is simply a data structure that describes the hardware. It provides a language for decoupling the hardware configuration from the board and device driver support in the Linux Kernel. Using it allows board and device support to become data driven; to make setup decisions based on data passed into the kernel instead of on per-machine hard coded selections. Applying for LFX mentorship I applied through the LFX mentorship portal where for a particular section you can apply to a maximum of three projects where first you have to create your profile, tell a bit about your background and then apply to the projects you want. I decided to apply for Linux Kernel Mentorship Programme Spring 2023. Here I had to go through a list of different tasks which tests your current knowledge about Linux Kernel, Command-line tools, etc. After completing those tasks and submitting my resume, I got my selection email for being selected as a mentee for Linux Kernel Mentorship Program My 12 Weeks Mentorship Journey My mentorship period was from 1st March 2023 to 31st May 2023. During this period I had to work with Linux Kernel where I had to pick among different subsystems and fix bugs on them. My mentor, Shuah Khan introduced us with a list of debugging techniques and ways to resolve them. This included using various command-line tools like GDB, event tracing, dynamic analysis of programs, and also the in-famous Google’s Syzkaller. With the help of my mentor, I learnt about various subsystems in the linux kernel, from where I chose to contribute to the conversion of Device Tree Bindings to YAML format. Every week, I used to have a meeting with my mentors other than having discussions on the communication channel with other mentees where we discussed about our subsystems and policies. And finally, I ended up submitting 13 patches as part of the mentorship program. In the End Apart from the bug fixing, I also learned few other interesting things about the Linux kernel and its developer community, like how we test various changes in the kernel and why we strictly use plain text emails. My mentorship program experience has been fantastic, and I recommend it to everyone interested in pursuing Linux Kernel development and looking for mentorsing. I am heartily thankful to my mentor Greg KH, Shuah Khan and The Linux Foundatioon, for providing me with this opportunity and a great learning experience. Patches can be found on https://lore.kernel.org/lkml/?q=danascape Let me know your experience with reading this blog at my email." }, { "title": "Introduction to AOSP", "url": "/posts/Introduction-to-AOSP/", "categories": "AOSP", "tags": "AOSP, Android", "date": "2022-11-06 00:00:00 +0530", "content": "The Android Open Source Project is an open source development project managed by Google, and anyone is welcome to evaluate and add code and fixes to the source code. Android System Architecture Let’s first talk about the layers of an Android device’s architecture before delving into the build system and the source code. Application Layer: This layer includes built-in/system applications for direct end-user interaction. Application Framework: This layer comprises of libraries that are created and run in the background, and are made accessible by Android interfaces, giving developers the API they need to create application’s individual parts. Some libraries present in an application framework are: Activity Manager: Oversees the life-cycle of each application (activity). Location Manager: Includes positioning services and geographic layers. Package Manager: Responsible for application specific actions, involving installation, permissions, and so on. Resource Manager: Provides the application the different resources it needs, including translated strings, images, layout files, color files, and so on. Notification Manager: Allows the application to display personalised display prompt information in the status bar. Telephony Manager: Controls all the mobile device features features, including the radio and SIM. Windows Manager: Controls how windows operate, ensuring that each activity opens in a new window. Content Providers: Enables data sharing between different applications. View System: Creating crucial view elements for an application, such as inflating the layout and displaying views on the screen. System Runtime Layer (Native): This consists of 2 layers: C/C++ Libraries: They provide services through the applications framework. Runtime Libraries: Core Library: It provides most of the core library functions so that developers can use the language to write applications. Virtual Machines: Java Virtual Machine (JVM) is a virtual machine capable of executing Java bytecode regardless of the base platform. The Java bytecode can run on any machine capable of supporting JVM. The Java compiler converts .java files into class files (bytecode). The bytecode is passed to JVM, which compiles it into machine code for execution directly on the CPU. Dalvik Virtual Machine (DVM) is a Java virtual machine developed and written by Dan Bornstein and others as a part of the Android mobile platform. Dalvik is a runtime for Android Operating System components and user applications. It is a virtual machine optimised for Android devices. It optimises the virtual machine for memory, battery life and performance. It allows multiple instances of VM to run in limited memory simultaneously. Each application is executed as an independent Linux process. Hardware Abstraction Layer (HALs) It is the interface between the Operating System Kernel and the hardware circuit to abstract or bring out the features of the hardware. It is used to control hardware using syscalls or proc values created by the kernel. It is generic and can be transplanted on various platforms. Linux Kernel Core system services depend or run on top of the Linux kernel. Android specific drivers are added on the base Linux kernel source for these core services. The system security, memory management, process management, network protocol stack and drivers models, etc. Android Boot Process The Boot ROM code starts executing from a pre-defined location which is hard-coded inside the ROM (chipset). It loads the bootloader into the RAM and starts executing. The bootloader runs in 2 stages: It detects the external RAM and loads programs which helps in executing the second stage. The bootloader sets up the network and memory, along with a basic environment to run the Linux kernel. The bootloader is able to provide configuration parameters or inputs to the kernel for its specific purpose. The Android Linux kernel is launched to set up cache and scheduling, loading of drivers, and so on. It then calls /init which is the root of the very first process. init is responsible for setting up kernel mounts like /sys, /dev and /proc. It then executes init.rc to proceed for further processing. Zygote is a virtual machine process that starts at system boot. It pre-loads and initializes core library classes, and enables code-sharing among the Dalvik VM. Zygote also launches system services. It forks new processes to launch the system services, some of which are: Power manager Activity Manager Telephony registry Package manager Content manager Zygote is also responsible for launching: Status bar services Hardware services Connectivity services Notification manager Corner System of Android Inter-Process Communication (IPC) It is a framework for the exchange of signals and data across multiple processes. It is used for passing of messages, synchronisation, shared memory and Remote Procedure Calls (RPCs). It enables information sharing, computational speed-up, modularity, convenience, privilege separation, data isolation and stability. Processes in Android are isolated and have a separate address space. One process cannot directly access another process’ memory; however if a process wants to offer some useful services to another process in order to discover or interact with those services, IPC can be used. Why the Binder? The existing IPC mechanisms were not reliable enough. Binder was a new IPC mechanism was introduced which abstracts the low-level details of IPC from the developer, allowing the applications to easily talk to both the system server and other remote service components. How was it implemented? As discussed before, a process cannot access another process’ memory directly; however the kernel has control over all processes and can therefore expose an interface that enables IPC. The /dev/binder device created by the binder kernel drivers is the central object of the framework and all IPC calls go through it. The binder driver manages part of the address space of each process. The memory is real-only to the process, and all the writing is all done by the kernel. When a process sends a message to another process, the kernel allocates some space in the destination process memory and copies the message data directly from the sender process. It then queues a message to the receiving process telling it where the received message is. The recipient can then access that message directly which happens in its own memory space. Establishing a Build Environment To build AOSP, a Linux build machine is recommended, with the following minimal specifications: ~16-cores, 32GB of RAM and around 300GB of free disk space, as per the AOSP docs. To setup the Linux build environment , you can directly run this command on your linux machine with elevated permissions: curl https://raw.githubusercontent.com/akhilnarang/scripts/master/setup/android_build_env.sh | sudo sh Downloading the Source Code We are going to build a Generic System Image, that is common to all devices. You can read more about GSI here. You can choose a source code of your choice. The base generic source code is uploaded by Google at android.googlesource.com. However, we also have LineageOS which is a fork of AOSP that also has generic image targets present (Credits to LineageOS developers). We are going to use the `repo tool that was installed during the setup we did in order to sync the source code. Check out the LineageOS manifest here to find the branch that you need to sync. Currently the generic targets for GSI are available in lineage-18.1 (based on android 11) and above. repo init -u https://github.com/LineageOS/android -b lineage-18.1 repo sync This will start downloading the source code, and if all goes well you will be able to see the directory structure after a successful source checkout. Building the Source Code Since we are selecting a generic target to build, let’s move forward with building an arm64 GSI for AB-type partitioning. To build the source code: First of all we have to setup the AOSP build environment: source build/envsetup.sh Then we have to lunch the required build target: lunch lineage_arm64_ab-userdebug Here lineage_arm64_ab is the product name and userdebug stands for user-debuggable. You can read more about the build types here. Now start the build by just running make This will start the build and will take a few hours to complete. The build time totally depends on your machine configuration – it can take upto few minutes to even a day! Emulating an Android Device After a successful build, the emulator path is added to the build path and can be run using the command: emulator After some time, you will be able to see a GUI interface that will boot the built images. What’s next? After the Operating System has booted, you can enjoy the appearance of pure AOSP. Some individuals could be unsure on what to do next. This is a normal question since it may have taken you some time to get here because utilising the AOSP build system is challenging and you may have made a common error. Once the OS has successfully booted, you can observe how a pure AOSP interface differs significantly from the UI on your Android smartphone. This is typical since Original Design Manufacturers (ODMs) use this source code as the starting point for later modifications to create the UI you adore on your smartphones. These ODMs improve the aesthetics of the AOSP framework, which handles UI management. These ODMs are also responsible for optimising the AOSP’s base components in order to provide a better user experience. The ODMs work on components that are used to specify the sort of device we are utilising as well as add hardware functionality support to the source code; these components are sometimes referred to as a device tree. The device configuration tree used by the AOSP build system contains board-specific changes like as partition sizes, feature flags, packages, configuration XMLs, properties, and so forth. These files are necessary for this generic source code to execute as reliably as or even more reliably than the device’s stock Operating System. Many of these files are already included in the source code and are produced by ODMs. These files allow us to create our own device tree, which we can then use to successfully build using the build system and boot the device. We also alter and employ these device trees in the creation of custom ROMs. You can try writing or finding the device tree of your own device that has an unlocked bootloader after becoming familiar with the AOSP build system. At this point, you will face challenges like working with and booting the device’s linux kernel or finding the configuration files that one requires. You can learn more about device trees and how to write one here. You can attempt delving into the XMLs inside the framework/base portion of the source code and studying them in order to adjust the layouts of the SystemUI or the QS tiles, the Notifications, and so on. If you are interested in the frontend/UI just like MIUI or OneUI do it. AOSP has its own XML-style layout, which will take some time getting accustomed to, but you will soon understand it and build on it as needed. There are other areas of the source code you can look into and learn more about, such the system libraries, core components, packages, and so forth (as mentioned in the architecture layers section). After gaining a basic idea of what AOSP is and how it functions, in addition to the cornerstone, which is the most crucial component of Android, you may start working your way through it. Let me know your experience with reading this blog at my email." }, { "title": "Android Device Tree Bringup", "url": "/posts/Android-Device-Tree-Bringup/", "categories": "AOSP", "tags": "AOSP, Android", "date": "2021-10-04 00:00:00 +0530", "content": "Learn how to bring-up a device tree for your device so you can get started with building your favourite custom ROM! Some of you guys might be wondering how people create your favorite custom ROMs like LineageOS, PixelExperience, HavocOS, and so on. I’m here to guide you about the basics of a device tree and how to do a scratch bring-up of your own. This article is a follow-up to my earlier article, Introduction to AOSP in which we discussed the fundamentals of and how to join the Android Open Source Project. Prerequisites Certain requirements are to be met before starting the bringup. Ability to understand basic editing/writing of files and text, familiarity with the command line interface and some basic git knowledge. A machine powerful enough to build Android – you can check the basic requirements here. Possess a fundamental understanding of the Android Open Source Project’s build system, commands, and other aspects. You can read more about it here. Note: This guide only applies to devices launched with Project Treble enabled (Android Oreo 8.0+). The device tree I’m using as a reference is a Qualcomm SDM632 (MSM8953) based tree, launched with Android 8.1. The guide uses LineageOS as the Android ROM Build System. Each ROM can have their own unique build system which might differ. The partition table of devices might differ from the device taken as an example below. Understanding Basic Structure Android device trees are certain bits and bytes of Makefile code that allows the Android build system to recognize the specifications of the target device, and what all is to be built along with it. After reading this guide, you will be able to bringup a prebuilt vendor based device tree (will explain what that means in a while). To start, let’s look at a basic bringup: These certain files and folders are ones we come across in every other device tree. Let’s break down the important ones: Android.mk: This is the most basic Makefile definition for any path to be included in the Android build system. This Makefile calls for all other Makefiles present in the all the directories and subdirectories of the device tree. The basic use of this specific Makefile is to guard the device tree Makefiles when not building for our target device. BoardConfig.mk: This contains the Board (SoC/Chipset) specific definition, such as the architecture (arm, arm64, x86, so on), the required build flags and the kernel build flags. These flags aid the build process. aosp_&lt;device-name&gt;.mk: This is a common Makefile found in most AOSP trees. This specific Makefile defines device name, manufacturer and build product details and is also used to call device.mk described below. AndroidProducts.mk: This Makefile includes the device product Makefiles, i.e. the aforementioned aosp_&lt;device-name&gt;.mk on lunch (term used to initialise device trees for build). This Makefile also defines the lunch target (Android 10+). device.mk: This Makefile contains all the build targets and copy specifications such as audio configs, feature configs, and so on. If you want a certain package to be built inline with Android ROM builds, this where you’d want to define it. The vendor tree Makefiles are also called from here. vendorsetup.sh: This shell script was superceded by AndroidProducts.mk after Android 10. Note: Not all the contents of a device tree are needed to be brought up from scratch since only certain files differ as per device and chipset, so I mentioned only the required ones. Starting the Bringup Now that we know the basic structure of an android device tree, let’s start our bringup. A prebuilt vendor based device tree is the one which uses device OEM vendor to run ROM. Basically while flashing ROMs based on this device tree you do not need to wipe vendor. You’ll need to figure out the codename of the device by looking at the build number or reading the build.prop or checking the stock rom dump. This is not important and you can use any codename for your device. P/S : My friend chose amogus as his device’s codename. You’ll also need to create a directory where your device tree will be placed and called by Android Build System. According to Google devices, you can create this directory in device/device-implementer/device-name/, where the device implementer is the device manufacturer, such as asus, google, oneplus, oppo, realme, xiaomi, and so on. Starting with Android.mk It must begin by defining the LOCAL_PATH variable such that android build system can call the makefile: LOCAL_PATH := $(call my-dir) Now we would need to guard our device Makefiles to only be called when it’s mentioned in lunch: ifeq ($(TARGET_DEVICE),&lt;device-codename&gt;) endif This means that it will start including the device tree only if the TARGET_DEVICE variable is set to your device. In order for our Android Makefile to include all other makefiles in the directory or sub directories, you can do it as: ifeq ($(TARGET_DEVICE),&lt;device-codename&gt;) include $(call all-makefiles-under,$(LOCAL_PATH)) endif In my case, the device’s codename is X01AD, so in the end it should look like this: LOCAL_PATH := $(call my-dir) ifeq ($(TARGET_DEVICE),X01AD) include $(call all-makefiles-under,$(LOCAL_PATH)) endif This concludes the basic Android.mk structure. Moving on to BoardConfig.mk As the name suggests, it includes board specific variables, such as chipset or device specific feature flags. There’s a certain set of flags that are important for all devices, such as the architecture, command line and partition information. Since I have an arm64 based Qualcomm device, I can start with setting up base architecture as such: TARGET_ARCH := arm64 TARGET_ARCH_VARIANT := armv8-a TARGET_CPU_ABI := arm64-v8a TARGET_CPU_ABI2 := TARGET_CPU_VARIANT := generic and a fallback secondary architecture as: TARGET_2ND_ARCH := arm TARGET_2ND_ARCH_VARIANT := armv8-a TARGET_2ND_CPU_ABI := armeabi-v7a TARGET_2ND_CPU_ABI2 := armeabi TARGET_2ND_CPU_VARIANT := generic The Android build server generates build artifacts and VNDK snapshot files using the following build parameters. You can read more about them here and here. For those who have an arm based device, you can just set: TARGET_ARCH := arm TARGET_ARCH_VARIANT := armv8-a TARGET_CPU_ABI := armeabi-v7a TARGET_CPU_ABI2 := armeabi TARGET_CPU_VARIANT := generic These are global variables and can be used for all devices. Developers might change CPU variants according to chipsets but that’s not necessary. Now, you need to define your device bootloader variables as such: TARGET_BOOTLOADER_BOARD_NAME := #(Based on your device) TARGET_NO_BOOTLOADER := true #(For source to not build bootloader) For my device it looks like this: Moving on to the kernel build system We will set up the required flags as per the LineageOS build system. In Android, Qualcomm (QCOM) and Mediatek (MTK) chipsets set some kernel parameters which are picked up by their Operating System (the preloaded stock ROM). They are set in device tree source as a kernel cmdline parameter this way: # You can have additional values to the same flag by using the append symbol (\"+=\") instead of initialise symbol (\":=\") BOARD_KERNEL_CMDLINE := #(This is initialised) BOARD_KERNEL_CMDLINE += You can obtain your device cmdline by unpacking your stock firmware bootimage using mkbootimg tools from here. During unpack you will obtain an output with kernel cmdline which looks like: ` cmd_line=’console=ttyMSM0,115200n8 androidboot.hardware=qcom androidboot.console=ttyMSM0 androidboot.memcg=1 lpm_levels.sleep_disabled=1 video=vfb:640x400,bpp=32,memsize=3072000 msm_rtb.filter=0x237 service_locator.enable=1 swiotlb=1 androidboot.usbcontroller=a600000.dwc3 earlycon=msm_geni_serial,0x880000 loop.max_part=7’ ` So after setting up your kernel cmdline you need to set some base kernel flags which matches your stock bootimage in order for it to boot up your device: BOARD_KERNEL_BASE := BOARD_KERNEL_PAGESIZE := BOARD_KERNEL_TAGS_OFFSET := BOARD_RAMDISK_OFFSET := They can be obtained from mkbootimg tools during unpack of your stock firmware bootimage. Read more about them here. After setting the flags, we want our kernel source to get built; as per the LineageOS build system we can either build our kernel from source or use prebuilt binaries as per our convenience. Usage of a source built kernel looks like this: TARGET_KERNEL_CONFIG := #(Kernel Config name to be picked up as per architecture from kernel-source-path/arch/&lt;device-architecture&gt;/configs/) TARGET_KERNEL_SOURCE := #(Your kernel source path from root of Android source) After setting up your flags it should look like this: BOARD_KERNEL_CMDLINE := console=ttyMSM0,115200,n8 androidboot.hardware=qcom BOARD_KERNEL_CMDLINE += androidboot.console=ttyMSM0 BOARD_KERNEL_CMDLINE += msm_rtb.filter=0x237 ehci-hcd.park=3 BOARD_KERNEL_CMDLINE += lpm_levels.sleep_disabled=1 BOARD_KERNEL_CMDLINE += androidboot.bootdevice=7824900.sdhci BOARD_KERNEL_CMDLINE += earlycon=msm_serial_dm,0x78af000 BOARD_KERNEL_CMDLINE += firmware_class.path=/vendor/firmware_mnt/image BOARD_KERNEL_CMDLINE += androidboot.usbconfigfs=true BOARD_KERNEL_CMDLINE += loop.max_part=7 BOARD_KERNEL_CMDLINE += androidboot.selinux=permissive BOARD_KERNEL_BASE := 0x80000000 BOARD_KERNEL_PAGESIZE := 2048 BOARD_KERNEL_IMAGE_NAME := Image.gz-dtb BOARD_KERNEL_TAGS_OFFSET := 0x00000100 BOARD_RAMDISK_OFFSET := 0x01000000 TARGET_KERNEL_CONFIG := X01AD_defconfig TARGET_KERNEL_SOURCE := kernel/asus/X01AD A configuration with prebuilt kernel should look like this: BOARD_KERNEL_CMDLINE := console=ttyMSM0,115200,n8 androidboot.hardware=qcom BOARD_KERNEL_CMDLINE += androidboot.console=ttyMSM0 BOARD_KERNEL_CMDLINE += msm_rtb.filter=0x237 ehci-hcd.park=3 BOARD_KERNEL_CMDLINE += lpm_levels.sleep_disabled=1 BOARD_KERNEL_CMDLINE += androidboot.bootdevice=7824900.sdhci BOARD_KERNEL_CMDLINE += earlycon=msm_serial_dm,0x78af000 BOARD_KERNEL_CMDLINE += firmware_class.path=/vendor/firmware_mnt/image BOARD_KERNEL_CMDLINE += androidboot.usbconfigfs=true BOARD_KERNEL_CMDLINE += loop.max_part=7 BOARD_KERNEL_CMDLINE += androidboot.selinux=permissive BOARD_KERNEL_BASE := 0x80000000 BOARD_KERNEL_PAGESIZE := 2048 BOARD_KERNEL_IMAGE_NAME := Image.gz-dtb BOARD_KERNEL_TAGS_OFFSET := 0x00000100 BOARD_RAMDISK_OFFSET := 0x01000000 TARGET_PREBUILT_KERNEL := device/asus/X01AD-kernel/Image.gz-dtb Source will pick up these offset flags for use in mkboot arguments, so there’s no need to set them again in the BoardConfig. Moving on to partition flags Different devices have different partition sizes and names. I will using my QCOM device as a reference, but it might be different for your device. Basically, a built Android OS flashable ZIP flashes over the boot, cache, recovery and system partitions. After Android 8.0, a new partition named vendor was introduced (Project Treble). #(Comment need to do something here and redirect over project treble) After more Android releases more partitions were introduced, so as per Android 11 convention, the partition used by Android are boot, cache, odm, product, recovery, system, system_ext, vendor. These are basic names and may change (like system_a, system_b for A/B devices). You can check path of your partitions using: ls -l /dev/block/bootdevice/by-name This path works for most of generic devices and may change. Now in order to obtain partition sizes blockdev --getsize64 &lt;partition-path&gt; Example: blockdev --getsize64 /dev/block/bootdevice/by-name/system This will give output of your partition sizes in bytes, you can directly set it as a flag variable so no need to convert the values. For prebuilt vendor based device trees, you can just fetch partition sizes for boot, cache, odm, product, recovery, system, system_ext, and userdata (/data). No need to worry if you do not have a certain partition – this list is tentative. After obtaining the partition sizes to set them as flags you can do this: BOARD_&lt;partition-name-in-capital&gt;IMAGE_PARTITION_SIZE := Example: BOARD_SYSTEMIMAGE_PARTITION_SIZE := 4294967296 After that you need to set partition filesystem type, this can be f2fs, ext2, ext3, ext4, and so on. You can find your partition filesystem type in your device fstab located in /vendor/etc/fstab.qcom To set partition file system type you can do this: BOARD_&lt;partition-name-in-caps&gt;IMAGE_FILE_SYSTEM_TYPE := Setting file system time for partitions is not important as the Android build system will pick default types if you do not set them, but the partition userdata is an exception where it is absolutely necessary to defined the filesystem type and the flags for doing so are different, namely: TARGET_USERIMAGES_USE_EXT4 := true # or TARGET_USERIMAGES_USE_F2FS:= true After setting up your partition flags and file system types, it should look like this: # Partitions - Boot BOARD_BOOTIMAGE_PARTITION_SIZE := 67108864 BOARD_FLASH_BLOCK_SIZE := 131072 # (BOARD_KERNEL_PAGESIZE * 64) # Partitions - Cache BOARD_CACHEIMAGE_PARTITION_SIZE := 367001600 BOARD_CACHEIMAGE_FILE_SYSTEM_TYPE := ext4 # Partitions - Recovery BOARD_RECOVERYIMAGE_PARTITION_SIZE := 67108864 # Partitions - System BOARD_SYSTEMIMAGE_PARTITION_SIZE := 4294967296 # Partitions - Userdata BOARD_USERDATAIMAGE_PARTITION_SIZE := 55423516160 Note: keep in mind that these are different as per devices. Moving on to Device Platform It can be set this way: TARGET_BOARD_PLATFORM := it can be msm8937, msm8953, sdm660, and so on. It is to be set according to your device. It is a CAF variable which is picked up by the LineageOS build system in order to build chipset specific HALs (We will learn about this later). Moving on to Recovery fstab Won’t explain much about it since you can read it here. So after reading the above thread, you can take your device fstab which can be found in /vendor/etc/fstab.qcom (fstab.qcom in my case, it can be anything else for other chipset devices, fstab.qcom is picked up by our bootloader) and set the flag pointing out it in device tree. TARGET_RECOVERY_FSTAB := fstab-path-in-device-source/fstab.qcom Example: TARGET_RECOVERY_FSTAB := $(DEVICE_PATH)/rootdir/etc/fstab.qcom Generating VNDK Snapshots Vendor Native Development Kit, check it out in brief here. You can set BoardConfig flags as read above and set the VNDK version as per your desired vendor. As a default behaviour, source sets BOARD_VNDK_VERSION := current, here current implies vndk version of current android source (like 29 for Q, 30 for R). Suppose you are trying to build Android Q for a vendor based on Android 9 – you can then set PRODUCT_TARGET_VNDK_VERSION := value as per your vendor (28 for Pie). You can also include additional VNDK snapshots for your vendor, like if you want Oreo vndk snapshots as well as Pie, you can set PRODUCT_EXTRA_VNDK_VERSIONS := value as you desire. SEPolicy / SELinux Setting SEPolicy is not very important for prebuilt vendor based device trees as most of selinux rules are defined in vendor sepolicy. You can read about SEPolicy here. Your device will proceed onto boot animation on SELinux due to the same reason as listed above, so you can just fixup the remaining denials by setting up SELinux on the system side. This can be done by including QCOM sepolicy (Board specific since this guide is for QCOM boards). BOARD_PLAT_PRIVATE_SEPOLICY_DIR += device/qcom/sepolicy/private BOARD_PLAT_PUBLIC_SEPOLICY_DIR += device/qcom/sepolicy/public This is global and can be used by any device, BOARD_PLAT implies SEPolicy will be updated in system side. Similarly BOARD_PRODUCT for product side. Calling out vendor BoardConfig Makefile Include your vendor device makefile as: # Inherit from the proprietary version include vendor/device-implementer/device-name/BoardConfigVendor.mk Example:- # Inherit from the proprietary version include vendor/asus/X01AD/BoardConfigVendor.mk We will learn more about vendor a bit later. Starting out with device product Source calls device product which relatively calls other Makefiles present in the device tree. By default when a product Makefile is called, it also includes the BoardConfig Makefile, so we don’t need to include it. A device product Makefile looks like ROM-name_device-name.mk, for example lineage_X01AD.mk. This can change accordingly for other ROMs. In the device product Makefile you can configure if you want 64/32-bit apps to run on your system, whether your device will have telephony (radio in basic words) or if it is wifi-only, and so on. These Makefiles are found in build/target/product in the source directory. According to basic Makefile syntax rules we can call another Makefile by using $(call inherit-product, &lt;makefile-path&gt;). In Android source, there is a variable which sets path for build/target, i.e $(SRC_TARGET_DIR), So in order to call a Makefile from target product we can just use $(SRC_TARGET_DIR)/product/&lt;makefile-name&gt;. You can read these Makefiles for more information. Device product Makefile is also used to call the ROM specific common Makefile (mainly ROM specific features). You can find these Makefiles in vendor/ROM-name/config/ which will be vendor/lineage/config in our case. We can call it this way: $(call inherit-product, vendor/lineage/config/common_full_phone.mk) In the end, we need to set device name and product name which should match our ROM-name. Example: # Device identifiers. PRODUCT_NAME := lineage_X01AD PRODUCT_DEVICE := X01AD PRODUCT_MANUFACTURER := asus PRODUCT_BRAND := asus PRODUCT_MODEL := ASUS_X01AD These are basic device identifiers and I don’t think further explanation is required since they’re fairly self-explanatory. Calling our device product Since Android Q, we need to create AndroidProducts.mk in order to call our device product makefile. This can be done this way: PRODUCT_MAKEFILES := \\ $(LOCAL_DIR)/ROM-name_device-name.mk Example: PRODUCT_MAKEFILES := \\ $(LOCAL_DIR)/lineage_X01AD.mk You can find a reference commit to set device product here. Last but not the least You need to call this Makefile from device product Makefile: $(call inherit-product, device/device-implementer/device-name/device.mk) Example: # Inherit from X01AD device $(call inherit-product, device/asus/X01AD/device.mk) We create a device makefile in our device tree which defines package names or device dimensions or screen density. Some basic flags are lister here: # Boot animation TARGET_SCREEN_HEIGHT := TARGET_SCREEN_WIDTH := # Device uses high-density artwork where available PRODUCT_AAPT_CONFIG := PRODUCT_AAPT_PREF_CONFIG := You can also inherit your device proprietary system files (will explain a bit later) as follows: $(call inherit-product-if-exists, vendor/device-implementer/device-name/device-name-vendor.mk) Customizing the build with overlays Android build system uses overlays to customize or override resource values specifically for a product. To use overlays, modify the project device Makefile to set PRODUCT_PACKAGE_OVERLAYS to a path relative to your top-level directory. You can do it as follows: PRODUCT_PACKAGE_OVERLAYS := device/device-implementer/device-name/overlay Default overlays can be found by extracting frameworks-res from your stock device dump using apktool (not in scope of this guide). Note that overlays are not important to boot the ROM on your device. They are used to overwrite certain properties or values over source to fixup certain bugs related to dimensions or similar. Read more about overlays and how they work here. Setting up device vendor There are certain libraries and APKs which are not source buildable (or proprietary) but are important for devices to have in order to gain some features or functionalities. These libraries or apps are listed in separate repo with proprietary files. In order to generate this repository, we extract the required proprietary libraries or apps from stock OEM firmware using proprietary extract scripts. These scripts are present in ROM side in certain ROMs which can be picked up from basic scripts which we set in our device trees, and which define certain variables and helps in generating a Makefile to ease our work and just call the repo in order to get started. At first you can pick these device-side shell scripts from here and modify them based on your device product name and manufacturer. Before running your shell scripts create a proprietary files list as per your device called proprietary-files.txt this way: ```txt ANT -lib/libantradio.so -lib64/libantradio.so product/lib/com.qualcomm.qti.ant@1.0.so product/lib64/com.qualcomm.qti.ant@1.0.so Alarm product/framework/vendor.qti.hardware.alarm-V1.0-java.jar Audio etc/permissions/audiosphere.xml framework/audiosphere.jar ``` This is a section of my current proprietary list and it can be different for other devices. You can find my full system-side listing here. Now, hoping that your list is created and all went well, in order to run extract-files.sh, you need adb support on your system or device dump where you can point out script to extract. You can read these scripts in order to check how they work and extract. After you run these scripts, it will extract the apps and libs listed in your proprietary files and will setup makefiles in a separate directory which can be called as listed above in BoardConfig and device makefile section. Getting things running If all above goes well, you can go to your ROM source directory and lunch the device product (I will not explain how to start build as its a basic requirement to know how to build a ROM before you do a bringup). Start your build and let it complete. These are only basic requirements for a device to get built and booting it is a different thing. Sometimes your ROM might get stuck on logo or stuck on endless boot animation, you will need to debug these issues in order to get things running. I will list up some reference system-only device trees that you can learn from. danascape/device_asus_X01AD danascape/device_oneplus_billie danascape/device_xiaomi_olive danascape/device_xiaomi_violet Certain device might need more things in order to get booting. These things might include AB or dynamic partition based BoardConfig flags, boot control HAL or gpt-utils or some extra libs. You can check more about how I did these things in above references. Let me know your new experiences at my email." }, { "title": "Writing an Overlayed DTS", "url": "/posts/Writing-an-Overlayed-DTS/", "categories": "Linux Kernel", "tags": "Linux, Kernel, Linux Kernel, DTS", "date": "2021-09-21 00:00:00 +0530", "content": "Overlayed device trees are written in order to avoid changing the base device tree structures already present in the kernel source, and just get them overridden during compilation. Learn how to create them! Basics A device tree is a data structure for describing hardware. Overlayed device trees are written in order to avoid changing the base device tree structures already present in the kernel source, and just get them overridden during compilation. This will involve modifying OEM device trees from their import commit to a fully overlayed device tree; as an example, from this to this. Steps 1. Find the initial device tree import for your device For example, in my case on the device Asus X01AD (sdm632), they were added in this commit. As you can see, most changes are additions done by the OEM. Now, grab a dmesg from the stock kernel, and find the name of the DTB that your stock kernel uses. We will need it to create our initial base device tree. You will find something like this in your dmesg (start the dmesg from 0’th second). Machine: Qualcomm Technologies, Inc. SDM632 PMI632 QRD SKU3 which implies our bootloader loads the dtb which points out to SDM632 PMI632 QRD SKU3 Board Name. 2. Make a clean base of your kernel which just boots For a clean kernel base, we add only the required changes on top of a base CAF (Code Aurora Forum) or ALS (Android Linux Stable) kernel source, which is enough to boot our device. This minimal change-set differs from device to device, some devices might just boot on dts (device tree structure) imports but some devices might need additional changes in power or video drivers, and hence we must add all these as per requirement. Before proceeding to write overlayed dts, I will consider that you have succeeded in booting your devices over a clean CAF or ALS base with minimal changes. As an example, my Asus X01AD took quite a few changes to boot; you can look into it here. 3. Get to know the device tree Now stalk (well, research or learn about) your dtsi imports so that you can know what the OEM changed over the base source – the additions and deletions, addition of new panels and battery data, and so on. 4. Create your DTS file Every device boots only on a single DTSI which points to the correct board name. In my case, it pointed to SDM632 PMI632 QRD SKU3 as we saw in point 1, and I found model = \"Qualcomm Technologies, Inc. SDM632 + PMI632 SOC\"; in sdm632.dts. As per my convenience, I named it sdm632-X01AD.dts – you can choose whatever name you want. This will be the DTS which includes other overlayed DTS. 5. Make our kernel build our new DTS You need to include the DTS in the Makefile of the dts directory – arch/arm64/boot/dts/qcom/Makefile, under the correct architecture name, ARCH_SDM632 in this case. You can call our DTS by inserting your dtsname.dtb under architecture, as seen in this commit. This is a bit different because I added guards in the Makefile to make sure only one DTB for my device is built, since it takes up space for no reason if the ones not needed are built too. 6. Organising the DTS You can now create a directory to organise the various DTS files if you want to (I called it X01AD). You can also create sub-directories if you want, mainly to divide display panel DTS or the battery data DTS and so on, this is all upto you. 7. Identifying the DSI panel and battery data DTSIs This step is for those who cannot recognise which dtsi are for DSI (Device Serial Interface) panels or battery data. To find them, you can check names of DTSIs in your OEM import. This works in most cases. Display panel DTSI will look like dsi-panel-\"name-of-driver\"-\"resolution\"-video.dtsi. Battery data DTSIs are different as per devices, so we will find them accordingly. In my case, I found dsi-panel-ili9881h-720p-video.dtsi for display and Huaqin_QL1830scud_4000mAh_averaged_MasterSlave_Sept25th2018_PMI632.dtsi for battery data. 8. Creating the base DTSI You now need to create a base DTSI in your custom directory and include it at the end of the .dts file we created in point 4. You can include it as #include \"your-custom-directory/base.dtsi\". My base dtsi is X01AD-base.dtsi and is included in this commit. Note that the base dtsi is created to bind up includes in a directory – it’s upto you if you want to include other DTSIs from your base DTS or use my way, they’re both fine. 9. Verifying our DTB builds You can try to build your kernel and check if it goes fine. You can skip this step if you’re confident about your work, it’s just to confirm if our includes are correct. In my case, I found my DTB built in out/arch/arm64/boot/dts. Now – this might be discouraging, but we’ve just begun; up until now, we haven’t finished 10% of the work. Battery data and DSI panel changes are just additions, so that’s the easiest part. 10. Checking out the changes in DTSI import commit For example, let’s take msm8953.dtsi as per this commit. Create a similar DTSI in your desired overlayed directory. This will make it easy for you and others to recall. In my case, it was X01AD-msm8953.dtsi. Now include this in your base overlay DTS or DTSI that you created in your device directory. 11. Bringing in changes Check the syntax in your OEM DTSI. In my case, the DTSI starts with / {. DTSIs can start with braces (as you saw above) or nodes – &amp;node. Now, let’s have a look at another example. In this line, the address of the other_ext region has been changed. In order to write that in our overlayed DTSI, write a new dtsi with / {, and follow up till that address, the other_ext_region. There’s no need to include other regions, adresses, or nodes, since our OEM has not modified them, leaving them as-is to be included from the base DTSI. This is an important step, since if it goes wrong, you will end up with weird cryptic errors from dtc. Our purpose can be to add, remove, or overwrite nodes, or to change regions, or add or remove properties. If you understood the above your dtsi should look as such: / { reserved-memory { other_ext_mem: other_ext_region@0 { It’s now time to write our updated property: reg = &lt;0x0 0x84a00000 0x0 0x1e00000&gt;; And now we just close the curly braces as per the syntax: / { reserved-memory { other_ext_mem: other_ext_region@0 { reg = &lt;0x0 0x84a00000 0x0 0x1e00000&gt;; }; }; }; There you go – you just made the same OEM changes overlayed in a new DTSI which will now be overridden onto the base DTSI during compilation. Other changes will also involve the same process, and you might need to delete nodes or properties as such: /delete-property/ property-name; /delete-node/ node-name; Here’s an example." } ]
